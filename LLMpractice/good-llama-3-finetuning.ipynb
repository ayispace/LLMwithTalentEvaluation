{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":67357,"databundleVersionId":8951125,"sourceType":"competition"},{"sourceId":8622192,"sourceType":"datasetVersion","datasetId":5123959},{"sourceId":33551,"sourceType":"modelInstanceVersion","modelInstanceId":28083,"modelId":39106}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Llama 3 8B Fine-tuning on the ARC Dataset\nIn this notebook, we will demonstrate how to fine-tune the instruct version of Llama 3 8B using Kaggle hardware. If you aim to apply a Large Language Model (LLM) to the ARC dataset and enhance its performance from its original state without relying on in-context learning, prompt engineering, or other techniques, this straightforward approach is for you.\n\nWe will utilize Q-Lora with low-rank adaptation to ensure compatibility with the hardware limitations on Kaggle. It is important to note that this fine-tuning process is not optimized and may not solve any tasks from the hidden test set. This notebook serves as a demonstration of how LLMs can be fine-tuned and the necessary packages required for the process.\n\nWe welcome any feedback you may have and appreciate your insights.\n\nIf there are any additional details you’d like to include or further adjustments needed, let us know!","metadata":{}},{"cell_type":"markdown","source":"## 1. Add datasets and Model\nWe will be using the following datasets:\n\n1. ‘ARC Prize 2024’: This is the official dataset containing the ARC tasks to be solved.\n2. (Optional) ‘Llama-3-ARC-deps’: This dataset contains the wheel files for additional packages not available in the Kaggle Kernel. Note that this dataset is required if you plan to submit this notebook to the competition, as no internet access is allowed during the competition. !missing! (not public)\n\nAdditionally, we need to add the original Llama 3 8B model:\n\n3. ‘Llama 3 8B-chat-hf’\n\nPlease note that to access the Llama 3 model on Kaggle, you need to obtain access from Meta. Instructions on how to do this can be found [here](https://www.kaggle.com/models/metaresearch/llama-3).","metadata":{}},{"cell_type":"markdown","source":"## 2. Install and Import Packages, and Log in to Weights & Biases (wandb)\n\nAs mentioned, we will be using Huggingface libraries, and most of the necessary packages are already available in Kaggle kernels. However, there are a few packages that are not included by default. If you are not submitting to the competition, you can download these packages directly.\n\nFor competition submissions, where internet access is restricted, we will use a Kaggle dataset containing the required wheel files. This allows us to install the packages without needing internet access during the submission process.\n\nAdditionally, we will log in to Weights & Biases (wandb) to track the progress of our fine-tuning process.","metadata":{}},{"cell_type":"markdown","source":"### 2.1 With internet access:\n\nIf we have internet access we can just directly install the packages:","metadata":{}},{"cell_type":"code","source":"!pip install -q -U -i https://pypi.org/simple/ bitsandbytes\n!pip install -q -U trl\n!pip install -q -U peft","metadata":{"execution":{"iopub.status.busy":"2024-10-29T00:20:01.699595Z","iopub.execute_input":"2024-10-29T00:20:01.700305Z","iopub.status.idle":"2024-10-29T00:20:44.791071Z","shell.execute_reply.started":"2024-10-29T00:20:01.700245Z","shell.execute_reply":"2024-10-29T00:20:44.789852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2 Without internet access (use for submission):\n\nIf we don't have internet access you can:\n1. Add the dataset we prepared [dataset name] !missing!\n2. Create your own dataset. You can find the explanation here [link] !missing!","metadata":{}},{"cell_type":"code","source":"deps_path = '/kaggle/input/llama-3-arc-deps'\n! pip install --no-index --find-links {deps_path} --requirement {deps_path}/requirements.txt","metadata":{"execution":{"iopub.status.busy":"2024-10-29T00:20:44.793416Z","iopub.execute_input":"2024-10-29T00:20:44.793825Z","iopub.status.idle":"2024-10-29T00:20:56.805201Z","shell.execute_reply.started":"2024-10-29T00:20:44.793779Z","shell.execute_reply":"2024-10-29T00:20:56.804076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.3 Import Packages\n\nNow, let’s import the necessary packages:","metadata":{}},{"cell_type":"code","source":"# For dataset\nimport pandas as pd\nimport json\nimport os\nimport ast\nimport re\nimport numpy as np\nfrom datasets import Dataset\nimport matplotlib.pyplot as plt\n\n# For LLM\nfrom peft import LoraConfig, PeftModel\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    set_seed,\n    pipeline\n)\nfrom trl import SFTTrainer, setup_chat_format, SFTConfig\n\nimport torch\nfrom time import time\n\n# For wandb\nfrom kaggle_secrets import UserSecretsClient\nimport wandb\n# Set seed\nset_seed(42)","metadata":{"execution":{"iopub.status.busy":"2024-10-29T00:20:56.806627Z","iopub.execute_input":"2024-10-29T00:20:56.806966Z","iopub.status.idle":"2024-10-29T00:21:21.621273Z","shell.execute_reply.started":"2024-10-29T00:20:56.806930Z","shell.execute_reply":"2024-10-29T00:21:21.620523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.4 Log in to Weights & Biases [optional] (wandb)\n\nTo log in to Weights & Biases (wandb), follow these steps:\n\n1. Add Your API Key as a Kaggle Secret:\n    - Navigate to the “Add-ons” section in the right-hand panel of your Kaggle notebook interface.\n    - Select “Secrets”.\n    - Click on “Add a new secret”.\n    - Enter a name for your secret (e.g., WANDB_API_KEY) and paste your wandb API key in the value field.\n    - Save the secret.\n2. Log in to wandb in Your Notebook:\n    - Use the following code to log in to wandb using the secret you just added:\n3. Initialize wandb:\n    - Before starting your training or fine-tuning process, initialize wandb to track your experiment. Use the following code snippet to set up your wandb run:","metadata":{}},{"cell_type":"code","source":"# from kaggle_secrets import UserSecretsClient\n# user_secrets = UserSecretsClient()\n# secret_value_0 = user_secrets.get_secret(\"WANDB_API_KEY\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"user_secrets = UserSecretsClient()\nwandb_key = user_secrets.get_secret(\"WANDB_API_KEY\")\n! wandb login $wandb_key","metadata":{"execution":{"iopub.status.busy":"2024-10-29T00:21:27.952046Z","iopub.execute_input":"2024-10-29T00:21:27.952996Z","iopub.status.idle":"2024-10-29T00:21:31.334273Z","shell.execute_reply.started":"2024-10-29T00:21:27.952943Z","shell.execute_reply":"2024-10-29T00:21:31.333123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 3. Load the data\n\nNext, let’s load the ARC tasks:","metadata":{}},{"cell_type":"code","source":"\n# Prepare data for DataFrame\n\n# Load JSON data from the files\nwith open('/kaggle/input/arc-prize-2024/arc-agi_evaluation_challenges.json') as f:\n    challenges = json.load(f)\n\nwith open('/kaggle/input/arc-prize-2024/arc-agi_evaluation_solutions.json') as f:\n    solutions = json.load(f)\n\ndata = []\nfor file_name, grids in challenges.items():\n    train_grids = grids.get('train', [])\n    test_inputs = grids.get('test', [])\n    test_outputs = solutions.get(file_name, [])\n    # Transform test grids to lists of dicts with 'output' key\n    test_outputs_transformed = [{'output': grid} for grid in test_outputs]\n    # Combine test inputs and outputs in alternating manner\n    combined_tests = []\n    for test_input, test_output in zip(test_inputs, test_outputs_transformed):\n        combined_tests.append({'input': test_input['input'], 'output': test_output['output']})\n    data.append({\n            'file_name': file_name,\n            'train': train_grids,\n            'test_input': test_inputs,\n            'test_output': test_outputs_transformed,\n            'test': combined_tests\n    })\n\n# Create DataFrame\ndf = pd.DataFrame(data)\n\n# Display the DataFrame\nprint(df)","metadata":{"execution":{"iopub.status.busy":"2024-10-29T00:21:35.664267Z","iopub.execute_input":"2024-10-29T00:21:35.664667Z","iopub.status.idle":"2024-10-29T00:21:35.897684Z","shell.execute_reply.started":"2024-10-29T00:21:35.664629Z","shell.execute_reply":"2024-10-29T00:21:35.896726Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 4. Load finetuned Llama-3 Model\n\nNext, we will load our fine-tuned Llama 3 model. We are using a 4-bit quantized version to reduce memory requirements. Ensure that you have selected an appropriate accelerator (T4x2) for the session, as sufficient memory is crucial for the training process to work effectively.","metadata":{}},{"cell_type":"code","source":"# Define a template for formatting chat messages with the Llama 3 model\n# This is model specific. Change it if you e.g. use Google's Gemma instead of Llama\nLLAMA_3_CHAT_TEMPLATE = \"\"\"{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\"\"\"\n\n# Set the data type for computations to float16, bfloat16 not supported on T4/P100\ncompute_dtype = getattr(torch, \"float16\")\n\n# Configure the BitsAndBytes settings for 4-bit quantization to reduce memory usage\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,  # Enable 4-bit quantization\n    bnb_4bit_use_double_quant=True,  # Use double quantization for improved precision\n    bnb_4bit_quant_type=\"nf4\",  # Specify the quantization type\n    bnb_4bit_compute_dtype=compute_dtype,  # Set the computation data type\n)\n\n# Specify the model ID change this if you e.g. want to try with Google's Gemma\nmodel_id = \"/kaggle/input/llama-3/transformers/8b-chat-hf/1\"\n\n# Record the start time to measure the loading duration\ntime_start = time()\n\n# Load the pre-trained model with specified configurations\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_id,\n    quantization_config=bnb_config,  # Apply the 4-bit quantization configuration\n    torch_dtype=compute_dtype,  # Set the data type for the model\n    use_cache=False,  # Disable caching to save memory\n    device_map='auto',  # Automatically map the model to available devices (e.g., GPUs)\n)\n\n# Enable gradient checkpointing to reduce memory usage during backpropagation\nmodel.gradient_checkpointing_enable()\n\n# Load the tokenizer associated with the model\ntokenizer = AutoTokenizer.from_pretrained(model_id)\ntokenizer.pad_token = tokenizer.eos_token  # Set the padding token to the end-of-sequence token you could also introduce a special pad token but this is not needed.\ntokenizer.chat_template = LLAMA_3_CHAT_TEMPLATE  # Apply the chat message template\n\n# Record the end time and print the duration for preparing the model and tokenizer\ntime_end = time()\nprint(f\"Prepare model, tokenizer: {round(time_end-time_start, 3)} sec.\")","metadata":{"execution":{"iopub.status.busy":"2024-10-29T00:21:40.934608Z","iopub.execute_input":"2024-10-29T00:21:40.935323Z","iopub.status.idle":"2024-10-29T00:22:57.542464Z","shell.execute_reply.started":"2024-10-29T00:21:40.935284Z","shell.execute_reply":"2024-10-29T00:22:57.541523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5. Create Prompts and filter the dataset\n\nNext, we will create the prompts that will be used to evaluate the model on the ARC dataset.","metadata":{}},{"cell_type":"markdown","source":"### 5.1 Create Prompts","metadata":{}},{"cell_type":"code","source":"# The system_prompt defines the initial instructions for the model, setting the context for solving ARC tasks.\nsystem_prompt = '''You are a puzzle solving wizard. You are given a puzzle from the abstraction and reasoning corpus developed by Francois Chollet.'''\n\n# User message template is a template for creating user prompts. It includes placeholders for training data and test input data, guiding the model to learn the rule and apply it to solve the given puzzle.\nuser_message_template = '''Here are the example input and output pairs from which you should learn the underlying rule to later predict the output for the given test input:\n----------------------------------------\n{training_data}\n----------------------------------------\nNow, solve the following puzzle based on its input grid by applying the rules you have learned from the training data.:\n----------------------------------------\n[{{'input': {input_test_data}, 'output': [[]]}}]\n----------------------------------------\nWhat is the output grid? Only provide the output grid in the form as in the example input and output pairs. Do not provide any additional information:'''\n\ndef preprocess(task, train_mode=True):\n    \"\"\"\n    Preprocess a single ARC task to create the prompt and solution for the model.\n\n    This function formats the system and user messages using a predefined template and the task's training and test data.\n    If in training mode, it also includes the assistant's message with the expected output.\n\n    Parameters:\n    task (dict): The ARC task data containing training and test examples.\n    train_mode (bool): If True, includes the assistant's message with the expected output for training purposes.\n\n    Returns:\n    dict: A dictionary containing the formatted text prompt, the solution, and the file name.\n    \"\"\"\n    # System message\n    system_message = {\"role\": \"system\", \"content\": system_prompt}\n\n    # Extract training data and input grid from the task\n    training_data = task['train']\n    input_test_data = task['test'][0]['input']\n    output_test_data = task['test'][0]['output']\n\n    # Format the user message with training data and input test data\n    user_message_content = user_message_template.format(training_data=training_data, input_test_data=input_test_data)\n    user_message = {\n        \"role\": \"user\",\n        \"content\": user_message_content\n    }\n\n    # Include the assistant message with the expected output if in training mode\n    if train_mode:\n        assistant_message = {\n            \"role\": \"assistant\",\n            \"content\": str(output_test_data)\n        }\n\n        # Combine system, user, and assistant messages\n        messages = [system_message, user_message, assistant_message]\n    else:\n        messages = [system_message, user_message]\n    # Convert messages using the chat template for use with the instruction finetuned version of Llama\n    messages = tokenizer.apply_chat_template(messages, tokenize=False)\n    return {\"text\": messages, \"solution\": output_test_data, \"file_name\": task['file_name']}\n\n# Convert the loaded data to a Huggingface Dataset object\ndataset = Dataset.from_pandas(df)\ndataset = dataset.shuffle(seed=42)\n# Split dataset into training and testing\ndataset = dataset.train_test_split(test_size=0.2)\n\n# Use the map method to apply the preprocess function\ndataset = dataset.map(preprocess, batched=False, remove_columns=dataset[\"train\"].column_names)","metadata":{"execution":{"iopub.status.busy":"2024-10-29T00:23:07.474082Z","iopub.execute_input":"2024-10-29T00:23:07.474824Z","iopub.status.idle":"2024-10-29T00:23:09.005421Z","shell.execute_reply.started":"2024-10-29T00:23:07.474778Z","shell.execute_reply":"2024-10-29T00:23:09.004489Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Check sample\nprint(dataset['train'][0]['text'])","metadata":{"execution":{"iopub.status.busy":"2024-10-29T00:23:20.400944Z","iopub.execute_input":"2024-10-29T00:23:20.401814Z","iopub.status.idle":"2024-10-29T00:23:20.408440Z","shell.execute_reply.started":"2024-10-29T00:23:20.401758Z","shell.execute_reply":"2024-10-29T00:23:20.407211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.2 Inspect the Prompts\n\nTo understand how many tasks we can consider for fine-tuning, we will inspect the number of tokens in the prompts for each task. This will give us an idea of the token length distribution and help us determine the feasibility of including various tasks within the model’s context window.","metadata":{}},{"cell_type":"code","source":"# Tokenize the dataset and store tokenized samples\ndata = dataset.map(lambda samples: tokenizer(samples['text']), batched=False)\n\ndef plot_data_lengths(tokenized_train_dataset, tokenized_val_dataset):\n    \"\"\"\n    Plot the distribution of token lengths in the training and validation datasets.\n\n    This function calculates the length of tokenized input texts in both the training and \n    validation datasets, combines the lengths, and plots a histogram to visualize their distribution.\n\n    Parameters:\n    tokenized_train_dataset (Dataset): The tokenized training dataset.\n    tokenized_val_dataset (Dataset): The tokenized validation dataset.\n\n    Returns:\n    None\n    \"\"\"\n    # Calculate the lengths of tokenized texts in the training dataset\n    lengths = [len(x['text']) for x in tokenized_train_dataset]\n    # Add the lengths of tokenized texts in the validation dataset\n    lengths += [len(x['text']) for x in tokenized_val_dataset]\n    \n    # Print the total number of lengths calculated\n    print(len(lengths))\n\n    # Plotting the histogram of token lengths\n    plt.figure(figsize=(10, 6))\n    plt.hist(lengths, bins=50, alpha=0.7, color='blue')\n    plt.xlabel('Length of input_ids')\n    plt.ylabel('Frequency')\n    plt.title('Distribution of Lengths of input_ids')\n    # Uncomment the line below to set x-axis limits, if needed\n    # plt.xlim([0, 1500])\n    plt.show()\n\n# Plot the distribution of token lengths in the training and validation datasets\nplot_data_lengths(data['train'], data['test'])","metadata":{"execution":{"iopub.status.busy":"2024-10-29T00:23:29.082541Z","iopub.execute_input":"2024-10-29T00:23:29.083363Z","iopub.status.idle":"2024-10-29T00:23:37.561613Z","shell.execute_reply.started":"2024-10-29T00:23:29.083323Z","shell.execute_reply":"2024-10-29T00:23:37.560656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 5.2 Filter the Dataset\n\nTo address memory limitations, we will restrict the dataset to tasks with prompt lengths of less than 2048 tokens. This ensures that the tasks fit within the model’s context window during fine-tuning.","metadata":{}},{"cell_type":"code","source":"# Define the maximum number of tokens allowed\nmax_tokens = 2048\n\n# Function to calculate the number of tokens in a text\ndef count_tokens(text):\n    \"\"\"\n    Calculate the number of tokens in a given text using the tokenizer.\n\n    Parameters:\n    text (str): The input text to be tokenized.\n\n    Returns:\n    int: The number of tokens in the input text.\n    \"\"\"\n    return len(tokenizer.encode(text))\n\n# Filter the dataset to include only tasks with a number of tokens within the allowed limit\nfiltered_train_dataset = dataset['train'].filter(lambda x: count_tokens(x['text']) <= max_tokens)\nfiltered_eval_dataset = dataset['test'].filter(lambda x: count_tokens(x['text']) <= max_tokens)\n\n# Calculate the number of tasks filtered out\nfiltered_out_train_tasks = len(dataset['train']) - len(filtered_train_dataset)\nfiltered_out_eval_tasks = len(dataset['test']) - len(filtered_eval_dataset)\n\n# Print the number of tasks filtered out and the remaining tasks\nprint(f'{filtered_out_train_tasks} training tasks were filtered out because they exceed the {max_tokens} token limit.')\nprint(f'The filtered training dataset contains {len(filtered_train_dataset)} tasks for fine-tuning.')\nprint(f'{filtered_out_eval_tasks} evaluation tasks were filtered out because they exceed the {max_tokens} token limit.')\nprint(f'The filtered evaluation dataset contains {len(filtered_eval_dataset)} tasks for evaluation.')","metadata":{"execution":{"iopub.status.busy":"2024-10-29T00:23:45.406854Z","iopub.execute_input":"2024-10-29T00:23:45.407246Z","iopub.status.idle":"2024-10-29T00:23:49.920735Z","shell.execute_reply.started":"2024-10-29T00:23:45.407207Z","shell.execute_reply":"2024-10-29T00:23:49.919876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 5.3 Test original model","metadata":{}},{"cell_type":"code","source":"torch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)\npipeline = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    torch_dtype=torch.float16,\n    device_map=\"auto\"\n)\n\nterminators = [\n    pipeline.tokenizer.eos_token_id,\n    pipeline.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n]\n\n# pipeline(dataset['train'][0]['text'], max_new_tokens=1000, return_full_text=False)\n\nprompt = filtered_eval_dataset[0]['text']\n\noutputs = pipeline(\n    prompt,\n    max_new_tokens=512,\n    eos_token_id=terminators,\n    do_sample=True,\n    temperature=0.6,\n    top_p=0.9\n)\nprint(outputs[0][\"generated_text\"][len(prompt):])","metadata":{"execution":{"iopub.status.busy":"2024-10-29T00:23:57.073672Z","iopub.execute_input":"2024-10-29T00:23:57.074053Z","iopub.status.idle":"2024-10-29T00:24:24.358894Z","shell.execute_reply.started":"2024-10-29T00:23:57.074015Z","shell.execute_reply":"2024-10-29T00:24:24.357989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 6. Finetune the model\n\nIn this section, we will set up the necessary configurations and initiate the fine-tuning process for our model on the filtered ARC dataset. This involves preparing the training environment, defining the training parameters, and starting the fine-tuning process.","metadata":{}},{"cell_type":"markdown","source":"### 6.1 LoRA Configuration\n\nIn this subsection, we will configure the Low-Rank Adaptation (LoRA) settings for fine-tuning our model. LoRA is an efficient technique that allows us to adapt pre-trained language models to specific tasks by adding low-rank updates. This approach helps in reducing the number of trainable parameters and computational requirements, making it suitable for our setup.","metadata":{}},{"cell_type":"code","source":"# Configure LoRA (Low-Rank Adaptation) for fine-tuning the model\npeft_config = LoraConfig(\n        lora_alpha=64,  # Scaling factor for the low-rank matrices\n        lora_dropout=0.05,  # Dropout rate to apply to the low-rank matrices\n        r=4,  # Rank of the low-rank matrices\n        bias=\"none\",  # Type of bias to use (none, all, or some specific layers)\n        task_type=\"CAUSAL_LM\",  # Specify the type of task (e.g., CAUSAL_LM for causal language modeling)\n        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # List of modules to apply LoRA to\n                        \"gate_proj\", \"up_proj\", \"down_proj\"],\n)\n\n# Explanation of LoRA Configuration Parameters:\n# lora_alpha: Controls the scaling of the low-rank adaptation, helping to balance between original weights and the adapted ones.\n# lora_dropout: Introduces dropout to the low-rank adaptation matrices, aiding in regularization.\n# r: Defines the rank of the low-rank matrices, controlling the number of parameters added.\n# bias: Determines whether and where to apply bias in the adapted model.\n# task_type: Specifies the type of task for fine-tuning (CAUSAL_LM for causal language modeling in this case).\n# target_modules: Lists the specific modules of the model where LoRA will be applied, focusing the adaptation on critical components.","metadata":{"execution":{"iopub.status.busy":"2024-10-29T00:24:38.257817Z","iopub.execute_input":"2024-10-29T00:24:38.258204Z","iopub.status.idle":"2024-10-29T00:24:38.263937Z","shell.execute_reply.started":"2024-10-29T00:24:38.258143Z","shell.execute_reply":"2024-10-29T00:24:38.263037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 6.2 Training Arguments\n\nIn this subsection, we will define the training arguments for fine-tuning our model. These settings include various parameters that control the training process, such as batch size, learning rate, evaluation strategy, and more. Properly configuring these arguments is crucial for efficient and effective model training.","metadata":{}},{"cell_type":"code","source":"# Define the output directory for the fine-tuned model\noutput_dir=\"llama3_8b_arc_v01\"\n\nsft_config = SFTConfig(\n    output_dir=output_dir,  # Directory to save the fine-tuned model and checkpoints\n    eval_strategy=\"steps\",  # Evaluate the model at regular steps\n    do_eval=True,  # Perform evaluation during training\n    optim=\"paged_adamw_8bit\",  # Optimizer to use for training (paged AdamW with 8-bit precision)\n    per_device_train_batch_size=1,  # Training batch size per device\n    gradient_accumulation_steps=8,  # Accumulate gradients over multiple steps to effectively increase batch size\n    per_device_eval_batch_size=1,  # Evaluation batch size per device\n    log_level=\"debug\",  # Logging level (debug for detailed logs)\n    save_steps=250,  # Save model checkpoint every 100 steps\n    logging_steps=10,  # Log training metrics every step\n    learning_rate=8e-6,  # Learning rate for the optimizer\n    eval_steps=250,  # Evaluate the model every 100 steps\n    max_steps=750,  # Maximum number of training steps\n    num_train_epochs=3,  # Number of training epochs\n    warmup_steps=10,  # Number of warmup steps for learning rate scheduler\n    lr_scheduler_type=\"cosine\",  # Type of learning rate scheduler (cosine annealing)\n    fp16=True,  # Use 16-bit floating point precision for training\n    bf16=False,  # Do not use bfloat16 precision\n    max_grad_norm=0.3,  # Maximum gradient norm for gradient clipping\n    gradient_checkpointing=True,  # Use gradient checkpointing to save memory\n    gradient_checkpointing_kwargs={'use_reentrant':False},  # Arguments for gradient checkpointing\n    ######\n    dataset_text_field=\"text\", # The field in the dataset containing the text data\n    max_seq_length=max_tokens,  # The maximum sequence length for tokenization\n    packing=False,\n)","metadata":{"execution":{"iopub.status.busy":"2024-10-29T00:24:42.462980Z","iopub.execute_input":"2024-10-29T00:24:42.463644Z","iopub.status.idle":"2024-10-29T00:24:42.500458Z","shell.execute_reply.started":"2024-10-29T00:24:42.463590Z","shell.execute_reply":"2024-10-29T00:24:42.499650Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"### 6.3 Training the Model\n\nIn this subsection, we will set up the environment for training and use the SFTTrainer to fine-tune our model on the ARC dataset. This involves configuring the trainer with the model, datasets, PEFT configuration, tokenizer, and training arguments, and then initiating the training process.","metadata":{}},{"cell_type":"code","source":"# Enable Weights & Biases (wandb) for tracking the training process\nos.environ[\"WANDB_DISABLED\"] = \"false\"\nos.environ[\"WANDB_PROJECT\"] = \"llama3_8b_ARC\"","metadata":{"execution":{"iopub.status.busy":"2024-10-29T00:24:47.151178Z","iopub.execute_input":"2024-10-29T00:24:47.151543Z","iopub.status.idle":"2024-10-29T00:24:47.156017Z","shell.execute_reply.started":"2024-10-29T00:24:47.151510Z","shell.execute_reply":"2024-10-29T00:24:47.155084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)\n# Set up the SFTTrainer for fine-tuning the model\n# Define the output directory for the fine-tuned model\noutput_dir=\"llama3_8b_arc_v01\"\n\ntrainer = SFTTrainer(\n        model=model,  # The pre-trained model to be fine-tuned\n        train_dataset=filtered_train_dataset,  # The training dataset\n        eval_dataset=filtered_eval_dataset,  # The evaluation dataset\n        peft_config=peft_config,  # LoRA configuration for parameter-efficient fine-tuning\n        tokenizer=tokenizer,  # The tokenizer for the model\n        args=sft_config,  # Training arguments configuration\n)\n\n# Print the number of trainable parameters in the model\ntrainer.model.print_trainable_parameters()\n\n# Start the training process\ntrainer.train()\n\n# Save the fine-tuned model\ntrainer.model.save_pretrained(output_dir)\n\n# Save the tokenizer\ntokenizer.save_pretrained(output_dir)\n\nprint(f\"Model and tokenizer saved to {output_dir}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-29T00:24:55.308146Z","iopub.execute_input":"2024-10-29T00:24:55.309092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 7. Create a Dataset to Be Used for Inference\n\nTo use your fine-tuned model for inference or submission, follow the steps outlined below:\n\nOption 1: Use a Publicly Available Dataset/model\n\nWe have created a dataset that includes the required packages and fine-tuned model. You can access it directly using this [link].\n\nOption 2: Create Your Own Dataset\n\nIf you prefer, you can create a custom dataset with the fine-tuned model and tokenizer files. Follow these steps to create your own dataset:\n\n1. Run Your Notebook:\n    - Execute your notebook to save the fine-tuned model and tokenizer into your working directory. Make sure to click on “Save Version” to capture the output.\n2. Save the Output:\n    - After running the notebook, navigate to the “Dataset” tab in the Kaggle interface.\n3. Create a New Dataset:\n    - Click on “New Dataset”.\n    - Select “Notebook Output Files” as the data source.\n    - Choose the notebook you ran earlier. This will include the directory where you saved the fine-tuned model and tokenizer.\n    - Provide a name and description for your dataset.\n    - Complete the creation process by following the on-screen instructions. You can even keep it automatically in sync with your notebook if you’d like to add further packages later on.\n\nBy following these steps, you will have a dataset containing the fine-tuned model and tokenizer, enabling you to use them for inference without requiring internet access during the competition.","metadata":{}},{"cell_type":"markdown","source":"# Closing Remarks\n\nThis notebook provides a basic demonstration of how to fine-tune the Llama 3 model on the ARC dataset using Kaggle hardware. It’s important to note that this solution has not been optimized or iterated upon and is meant primarily to showcase the steps involved in fine-tuning an LLM and preparing it for inference.\n\nThe methods and configurations used here are quite straightforward and do not incorporate advanced techniques that could significantly improve performance. For example, chain-of-thought prompting, more sophisticated data augmentation, and extensive hyperparameter tuning were not employed in this demonstration.\n\nFor those interested in state-of-the-art (SOTA) performance on the ARC dataset using a fine-tuned LLM, I highly recommend exploring the work of Jack Cole. He has achieved SOTA results by using a much larger dataset and more advanced techniques, demonstrating the potential of fine-tuned LLMs when more resources and sophisticated methods are applied.\n\nWhile this notebook provides a starting point, achieving high performance on tasks like those in the ARC dataset typically requires a more thorough and nuanced approach. We encourage you to experiment further, iterate on these methods, and explore more advanced techniques to improve your model’s performance.\n\nBy following these steps and considerations, you can better understand the process and potential of fine-tuning large language models for specific tasks. Happy experimenting!","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}